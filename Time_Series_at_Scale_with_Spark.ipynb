{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Time_Series_at_Scale_with_Spark.ipynb","provenance":[],"mount_file_id":"17WB4URrt6ypAF3YwuG1lRhE7_XvcXlQ_","authorship_tag":"ABX9TyP5CmDu+yXVpjhg+mS7QX5J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5. Implementation of Scalable Demand Forecasting with PySpark in Google Colab\n","Similar to setting up Prophet, PySpark installation can be very difficult at times. However, those tasks are extremely easy Google Colaboratory. \n","\n","First, go to <a href = \"https://research.google.com/colaboratory\">Google Colab</a> and click \"File\" -> \"New notebook\" to create a new notebook.\n","\n","### 5.1. Preparation\n","#### 5.1.1. Mount to Google Drive\n","For easy access to files, connect the notebook to your Google Drive."],"metadata":{"id":"XDqaPV1K0syV"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scRRK2ujaFyM","executionInfo":{"status":"ok","timestamp":1651265208986,"user_tz":420,"elapsed":1292,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"ba25ab2e-3026-436b-9917-50834d442446"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Import library\n","from google.colab import drive\n","\n","# Connect to your google drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#### 5.1.2. Install PySpark and Prophet\n","Installing PySpark and Prophet only require one line of code for each."],"metadata":{"id":"e_UTJL8k1uZP"}},{"cell_type":"code","source":["# Install PySpark\n","!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UjTNhmwYaLZL","executionInfo":{"status":"ok","timestamp":1651265214005,"user_tz":420,"elapsed":5021,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"43b38c11-e996-4012-bd10-0f6a132e90e7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n","Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"]}]},{"cell_type":"code","source":["# Install Prophet                                                                                                                                                                                                  \n","!pip install Prophet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhduj8z92YIC","executionInfo":{"status":"ok","timestamp":1651265218184,"user_tz":420,"elapsed":4192,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"826b35c5-b857-4180-db5e-64fdde56cfe8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Prophet in /usr/local/lib/python3.7/dist-packages (1.0.1)\n","Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.10.5.2)\n","Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (4.64.0)\n","Requirement already satisfied: cmdstanpy==0.9.68 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.9.68)\n","Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.3.5)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.21.6)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.8.2)\n","Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.4.0)\n","Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.0.9)\n","Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.2)\n","Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.29.28)\n","Requirement already satisfied: pystan~=2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.19.1.1)\n","Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from cmdstanpy==0.9.68->Prophet) (5.2.0)\n","Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->Prophet) (0.5.11)\n","Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (2.2.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (1.15.0)\n","Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (0.2.1)\n","Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (4.1.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (2022.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (3.0.8)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->Prophet) (4.2.0)\n"]}]},{"cell_type":"markdown","source":["#### 5.1.3. Load necessary packages"],"metadata":{"id":"u4KOO_ZeUp-C"}},{"cell_type":"code","source":["# Import library\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import pandas_udf, PandasUDFType\n","from pyspark.sql.types import *\n","from prophet import Prophet\n"],"metadata":{"id":"JMaP3Da-UzyI","executionInfo":{"status":"ok","timestamp":1651265219055,"user_tz":420,"elapsed":881,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#### 5.1.4. Upload the CSV file to Google Drive\n","- Click the folder icon in the left menu as shown in the image below.\n","- Although you can save anywhere you wish, I like to save it in the Google Drive Colab Notebook folder. To do so, go to \"content\" -> \"dive\" -> \"MyDrive\" -> Colab Notebooks -> create \"data\" folder\n","- Click the three dots next to \"data\". You can upload the CSV file we saved by clicking \"Upload\"\n","\n","<img src =\"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab1.JPG?raw=true\">\n","<img src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab2.JPG?raw=true\">\n","<img src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab3.JPG?raw=true\">\n","\n","#### 5.1.5. Import the CSVÂ file and explore it\n","Import the CSV file we just uploaded to Google Drive.\n"],"metadata":{"id":"T_7NDC3_PPhZ"}},{"cell_type":"code","source":["# Import the csv file and explore it\n","sales_pd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/store_sales.csv')\n","\n","# Convert ds to datetime\n","sales_pd['ds'] = pd.to_datetime(sales_pd['ds'])\n","\n","# Display info\n","sales_pd.info()"],"metadata":{"id":"64_fumPM2-lA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651265219851,"user_tz":420,"elapsed":798,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"73862f24-dbb9-423d-ad79-7521c27776ac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 913000 entries, 0 to 912999\n","Data columns (total 4 columns):\n"," #   Column  Non-Null Count   Dtype         \n","---  ------  --------------   -----         \n"," 0   ds      913000 non-null  datetime64[ns]\n"," 1   store   913000 non-null  int64         \n"," 2   item    913000 non-null  int64         \n"," 3   y       913000 non-null  float64       \n","dtypes: datetime64[ns](1), float64(1), int64(2)\n","memory usage: 27.9 MB\n"]}]},{"cell_type":"code","source":["# Descriptive statistics\n","sales_pd.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"aq2XcbmLWhh7","executionInfo":{"status":"ok","timestamp":1651265219851,"user_tz":420,"elapsed":4,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"f9e3eee0-edb9-4307-deaf-2502ffbf5678"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["               store           item              y\n","count  913000.000000  913000.000000  913000.000000\n","mean        5.500000      25.500000      52.250287\n","std         2.872283      14.430878      28.801144\n","min         1.000000       1.000000       0.000000\n","25%         3.000000      13.000000      30.000000\n","50%         5.500000      25.500000      47.000000\n","75%         8.000000      38.000000      70.000000\n","max        10.000000      50.000000     231.000000"],"text/html":["\n","  <div id=\"df-8d24e819-b4c2-4f43-acc1-41d42763117d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>store</th>\n","      <th>item</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>913000.000000</td>\n","      <td>913000.000000</td>\n","      <td>913000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>5.500000</td>\n","      <td>25.500000</td>\n","      <td>52.250287</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>2.872283</td>\n","      <td>14.430878</td>\n","      <td>28.801144</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>3.000000</td>\n","      <td>13.000000</td>\n","      <td>30.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>5.500000</td>\n","      <td>25.500000</td>\n","      <td>47.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>8.000000</td>\n","      <td>38.000000</td>\n","      <td>70.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>10.000000</td>\n","      <td>50.000000</td>\n","      <td>231.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d24e819-b4c2-4f43-acc1-41d42763117d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8d24e819-b4c2-4f43-acc1-41d42763117d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8d24e819-b4c2-4f43-acc1-41d42763117d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Looks like we correctly have 1-10 stores and 1-50 items.\n","\n","### 5.2. Prophet x PySpark\n","#### 5.2.1. Create a Spark session\n","Spark Sessions utilize Spark's functions. They are created in the Driver program, which is inside the Master node. \n","\n","Spark uses Master-Slave architecture. Salve nodes execute the tasks assigned by the Master node."],"metadata":{"id":"L23Aa-B7XTri"}},{"cell_type":"code","source":["# Create a Spark Session - Run it on a standalone mode since it is just a practice\n","# master(): Either yarn or mesos; local[X] when running in standalone\n","# appName(): Name of the application\n","# getOrCreate: returns existing SparkSession; otherwise, create a new one\n","spark = SparkSession.builder\\\n","        .master(\"local\")\\\n","        .appName(\"Colab\")\\\n","        .config('spark.ui.port', '4050')\\\n","        .getOrCreate()"],"metadata":{"id":"Ni1i9p58aMz1","executionInfo":{"status":"ok","timestamp":1651265232691,"user_tz":420,"elapsed":12842,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#### 5.2.2. Structure schema\n","After reading the CSV file with PySpark we will structure the output of the data. See <a href = \"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html\">here</a> for different types of struct fields.\n"],"metadata":{"id":"Onjwcxr9bc8V"}},{"cell_type":"code","source":["# Read the csv file\n","sales_df = spark.createDataFrame(sales_pd)\n","\n","# Display the schema\n","sales_df.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hF4EHn9TaPW9","executionInfo":{"status":"ok","timestamp":1651265309948,"user_tz":420,"elapsed":77268,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"73e5de30-7084-4f44-9029-d9f878bf910d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- ds: timestamp (nullable = true)\n"," |-- store: long (nullable = true)\n"," |-- item: long (nullable = true)\n"," |-- y: double (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# Define a schema\n","schema = StructType([\n","                     StructField('store', IntegerType()),\n","                     StructField('item', IntegerType()),\n","                     StructField('ds', TimestampType()),\n","                     StructField('y', FloatType()),\n","                     StructField('yhat', DoubleType()),\n","                     StructField('yhat_upper', DoubleType()),\n","                     StructField('yhat_lower', DoubleType()),\n","                     ])  "],"metadata":{"id":"OMmUC-UYhhBr","executionInfo":{"status":"ok","timestamp":1651265309948,"user_tz":420,"elapsed":13,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### 5.2.3. Utilize Pandas UDF and PySpark to train multiple models in parallel\n","The next step is to set parameters, fit the model, and predict sales just as we did for 1 forecast model. We are going to build a function and apply that function to all store-item groups. The only difference between this and our previous 1-model forecast is that we are going to utilize Pandas UDF and PySpark to parallelize the process."],"metadata":{"id":"vZYqQhhfh750"}},{"cell_type":"code","source":["# define the Pandas UDF \n","@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n","def apply_model(store_pd):\n","  \n","  # instantiate the model and set parameters\n","  model = Prophet(\n","      interval_width=0.95,\n","      growth='linear',\n","      daily_seasonality=False,\n","      weekly_seasonality=True,\n","      yearly_seasonality=True,\n","      seasonality_mode='multiplicative'\n","  )\n","  \n","  # fit the model to historical data\n","  model.fit(store_pd)\n","  \n","  # Create a data frame that lists 90 dates starting from Jan 1 2018\n","  future = model.make_future_dataframe(\n","      periods=90,\n","      freq='d',\n","      include_history=True)\n","  \n","  # Out of sample prediction\n","  future = model.predict(future)\n","\n","  # Create a data frame that contains store, item, y, and yhat\n","  f_pd = future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]\n","  st_pd = store_pd[['ds', 'store', 'item', 'y']]\n","  result_pd = f_pd.join(st_pd.set_index('ds'), on='ds', how='left')\n","  \n","  # fill store and item\n","  result_pd['store'] = store_pd['store'].iloc[0]\n","  result_pd['item'] = store_pd['item'].iloc[0]\n","  #result_pd['store'] = store_pd['store'].fillna(method='ffill')\n","  #result_pd['item'] = store_pd['item'].fillna(method='ffill')\n","  return result_pd[['store', 'item', 'ds', 'y', 'yhat',\n","                    'yhat_upper', 'yhat_lower']]"],"metadata":{"id":"a0xZR1Hbh5Pr","executionInfo":{"status":"ok","timestamp":1651265309949,"user_tz":420,"elapsed":11,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Apply the function to all store-items\n","results = sales_df.groupby(['store', 'item']).apply(apply_model)\n","\n","# Print the results - calculate the time to run\n","import timeit\n","start = timeit.default_timer()\n","results.show()\n","stop = timeit.default_timer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkAtY7_yrhBr","executionInfo":{"status":"ok","timestamp":1651265347700,"user_tz":420,"elapsed":37761,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"a6559214-43ce-4265-afa3-b11d9b4092a4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/group_ops.py:84: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n","  \"more details.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["+-----+----+-------------------+----+------------------+------------------+--------------------+\n","|store|item|                 ds|   y|              yhat|        yhat_upper|          yhat_lower|\n","+-----+----+-------------------+----+------------------+------------------+--------------------+\n","|    1|   1|2013-01-01 00:00:00|13.0|10.051272869689301| 19.81331333729659|  1.1653252427277994|\n","|    1|   1|2013-01-02 00:00:00|11.0|10.528625323821489| 19.52697394359962|  1.1040720751204967|\n","|    1|   1|2013-01-03 00:00:00|14.0|11.053264561305632|19.121703358223876|   2.340893727254274|\n","|    1|   1|2013-01-04 00:00:00|13.0|12.244392640789227|20.759074056045254|  3.1621289404368893|\n","|    1|   1|2013-01-05 00:00:00|10.0| 13.78033453999933|22.222193062919708|  4.8703122255888385|\n","|    1|   1|2013-01-06 00:00:00|12.0|14.378950515739104| 22.87331589821483|   5.523753221406483|\n","|    1|   1|2013-01-07 00:00:00|10.0| 7.872892467507367|16.531746667114465|-0.49527759234844837|\n","|    1|   1|2013-01-08 00:00:00| 9.0|  9.90131654595669|17.983202458380788|  1.5935210585451272|\n","|    1|   1|2013-01-09 00:00:00|12.0|10.315249013275794|18.817213347658306|   2.432942770254938|\n","|    1|   1|2013-01-10 00:00:00| 9.0|10.787017356538591|19.251265358946995|  2.3201248028187433|\n","|    1|   1|2013-01-11 00:00:00| 9.0|11.938934222254371|20.767379026927884|  3.5104766485024106|\n","|    1|   1|2013-01-12 00:00:00| 7.0|13.449630508550548|21.316673302937392|   4.987213972979563|\n","|    1|   1|2013-01-13 00:00:00|10.0| 14.03440099518179|22.688162887549456|    5.64841096988109|\n","|    1|   1|2013-01-14 00:00:00|12.0| 7.510442511052676|15.819486817555884| -0.8062669254682606|\n","|    1|   1|2013-01-15 00:00:00| 5.0| 9.556668240912103| 17.69174258147707| 0.14600151798088512|\n","|    1|   1|2013-01-16 00:00:00| 7.0| 9.997643135111675| 18.50684728429208|  1.7126476952800282|\n","|    1|   1|2013-01-17 00:00:00|16.0|10.509012136011599|19.386367232020806|  1.6553658294322011|\n","|    1|   1|2013-01-18 00:00:00| 7.0|11.713373568083002|20.512731380385276|   2.741445571481389|\n","|    1|   1|2013-01-19 00:00:00|18.0|13.286933306466477|22.032897863131133|   4.064891882674283|\n","|    1|   1|2013-01-20 00:00:00|15.0|13.939857891685119|  22.2289975872298|    4.63002332724694|\n","+-----+----+-------------------+----+------------------+------------------+--------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["# Print the time it took to forecast 500 models\n","print('Time: ', stop - start)   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HYMOUeKuKui","executionInfo":{"status":"ok","timestamp":1651265347701,"user_tz":420,"elapsed":11,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"1a13a265-41d0-4265-c5a9-5972ccce2593"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Time:  37.37365171300007\n"]}]},{"cell_type":"markdown","source":["It only took 29 seconds to train 500 models and forecast 3 months out!"],"metadata":{"id":"iQ3K4bl3-geO"}},{"cell_type":"code","source":["type(results) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_mm2XySFmSX","executionInfo":{"status":"ok","timestamp":1651265680667,"user_tz":420,"elapsed":309,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"}},"outputId":"7b4603d8-6b8d-47d5-830e-676d8fbc50b9"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# 6. Conclusion\n","In this long post, we went through several topics. We started with identifying trends and seasonality, moved on to building a Prophet model, and scaled the process to model 500 distinct models with PySpark. We didn't get to cover CNN, LSTM, and Seasonal ARIMA but I am planning on adding them in a few days."],"metadata":{"id":"GFWiCFe7-xEc"}}]}