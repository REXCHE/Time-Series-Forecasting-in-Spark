{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_Series_at_Scale_with_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Implementation of Scalable Demand Forecasting with PySpark in Google Colab\n",
        "Similar to setting up Prophet, PySpark installation can be very difficult at times. However, those tasks are extremely easy Google Colaboratory. \n",
        "\n",
        "First, go to <a href = \"https://research.google.com/colaboratory\">Google Colab</a> and click \"File\" -> \"New notebook\" to create a new notebook.\n",
        "\n",
        "### 5.1. Preparation\n",
        "#### 5.1.1. Mount to Google Drive\n",
        "For easy access to files, connect the notebook to your Google Drive."
      ],
      "metadata": {
        "id": "XDqaPV1K0syV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scRRK2ujaFyM",
        "outputId": "4f17b92d-4cb6-4043-9f0c-2865433e5497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import library\n",
        "from google.colab import drive\n",
        "\n",
        "# Connect to your google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.2. Install PySpark and Prophet\n",
        "Installing PySpark and Prophet only require one line of code for each."
      ],
      "metadata": {
        "id": "e_UTJL8k1uZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "\n",
        "# Unzip\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "\n",
        "# Install spark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "\n",
        "#\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "UjTNhmwYaLZL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Prophet                                                                                                                                                                                                  \n",
        "!pip install Prophet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhduj8z92YIC",
        "outputId": "977b40ca-1f1d-4e6e-faf1-7c8a5ec872fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Prophet\n",
            "  Downloading prophet-1.0.1.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.29.28)\n",
            "Collecting cmdstanpy==0.9.68\n",
            "  Downloading cmdstanpy-0.9.68-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pystan~=2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.19.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (3.2.2)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.10.5.2)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (4.64.0)\n",
            "Collecting ujson\n",
            "  Downloading ujson-5.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->Prophet) (0.5.11)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (0.2.1)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (2.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (1.15.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (4.1.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->Prophet) (4.2.0)\n",
            "Building wheels for collected packages: Prophet\n",
            "  Building wheel for Prophet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Prophet: filename=prophet-1.0.1-py3-none-any.whl size=6639859 sha256=1bed5f7a5cadf1e380640aada559bd26dae318aee1afc01acd6ce38910fb499d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/a0/1a/02c9ec9e3e9de6bdbb3d769d11992a6926889d71567d6b9b67\n",
            "Successfully built Prophet\n",
            "Installing collected packages: ujson, cmdstanpy, Prophet\n",
            "  Attempting uninstall: cmdstanpy\n",
            "    Found existing installation: cmdstanpy 0.9.5\n",
            "    Uninstalling cmdstanpy-0.9.5:\n",
            "      Successfully uninstalled cmdstanpy-0.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fbprophet 0.7.1 requires cmdstanpy==0.9.5, but you have cmdstanpy 0.9.68 which is incompatible.\u001b[0m\n",
            "Successfully installed Prophet-1.0.1 cmdstanpy-0.9.68 ujson-5.2.0\n",
            "Requirement already satisfied: Prophet in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.2)\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.29.28)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.3.5)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.4.0)\n",
            "Requirement already satisfied: pystan~=2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.19.1.1)\n",
            "Requirement already satisfied: cmdstanpy==0.9.68 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.9.68)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.21.6)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.0.9)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.10.5.2)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from cmdstanpy==0.9.68->Prophet) (5.2.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.1.2->Prophet) (0.5.11)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (2.2.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays>=0.10.2->Prophet) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (2022.1)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar>=0.0.9->Prophet) (4.1.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->Prophet) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->Prophet) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.3. Load necessary packages"
      ],
      "metadata": {
        "id": "u4KOO_ZeUp-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.types import *\n",
        "from prophet import Prophet\n"
      ],
      "metadata": {
        "id": "JMaP3Da-UzyI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.4. Upload the CSV file to Google Drive\n",
        "- Click the folder icon in the left menu as shown in the image below.\n",
        "- Although you can save anywhere you wish, I like to save it in the Google Drive Colab Notebook folder. To do so, go to \"content\" -> \"dive\" -> \"MyDrive\" -> Colab Notebooks -> create \"data\" folder\n",
        "- Click the three dots next to \"data\". You can upload the CSV file we saved by clicking \"Upload\"\n",
        "\n",
        "<img src =\"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab1.JPG?raw=true\">\n",
        "<img src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab2.JPG?raw=true\">\n",
        "<img src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab3.JPG?raw=true\">\n",
        "\n",
        "#### 5.1.5. Import the CSV file and explore it\n",
        "Import the CSV file we just uploaded to Google Drive.\n"
      ],
      "metadata": {
        "id": "T_7NDC3_PPhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the csv file and explore it\n",
        "sales_pd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/store_sales.csv')\n",
        "\n",
        "# Convert ds to datetime\n",
        "sales_pd['ds'] = pd.to_datetime(sales_pd['ds'])\n",
        "\n",
        "# Display info\n",
        "sales_pd.info()"
      ],
      "metadata": {
        "id": "64_fumPM2-lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39b48f7-0a20-4f3f-a45e-f2ea450c9413"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 913000 entries, 0 to 912999\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count   Dtype         \n",
            "---  ------  --------------   -----         \n",
            " 0   ds      913000 non-null  datetime64[ns]\n",
            " 1   store   913000 non-null  int64         \n",
            " 2   item    913000 non-null  int64         \n",
            " 3   y       913000 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
            "memory usage: 27.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics\n",
        "sales_pd.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "aq2XcbmLWhh7",
        "outputId": "a685be31-c685-4160-bd0f-a58a0ef32fd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               store           item              y\n",
              "count  913000.000000  913000.000000  913000.000000\n",
              "mean        5.500000      25.500000      52.250287\n",
              "std         2.872283      14.430878      28.801144\n",
              "min         1.000000       1.000000       0.000000\n",
              "25%         3.000000      13.000000      30.000000\n",
              "50%         5.500000      25.500000      47.000000\n",
              "75%         8.000000      38.000000      70.000000\n",
              "max        10.000000      50.000000     231.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a1dcd7a-fc4e-4a8f-a1b6-a6ef941a9b47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>store</th>\n",
              "      <th>item</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>913000.000000</td>\n",
              "      <td>913000.000000</td>\n",
              "      <td>913000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.500000</td>\n",
              "      <td>25.500000</td>\n",
              "      <td>52.250287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.872283</td>\n",
              "      <td>14.430878</td>\n",
              "      <td>28.801144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.500000</td>\n",
              "      <td>25.500000</td>\n",
              "      <td>47.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>70.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>231.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a1dcd7a-fc4e-4a8f-a1b6-a6ef941a9b47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a1dcd7a-fc4e-4a8f-a1b6-a6ef941a9b47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a1dcd7a-fc4e-4a8f-a1b6-a6ef941a9b47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.6. Increase the sample size by 10X\n",
        "Check the number of unique groups."
      ],
      "metadata": {
        "id": "JQzDYHu1latF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique store-items\n",
        "sales_pd[['item', 'store']].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swHT95b0wSYj",
        "outputId": "b2b8ae72-9cf8-4fa3-f202-1c179c534781"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "item     50\n",
              "store    10\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 50 items and 10 stores, which means that we have 500 unique groups. Let's increase this data set by ten folds by concatenating it by itself until the number of groups reaches 5,000."
      ],
      "metadata": {
        "id": "-KN2FK3kwXgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a larger data frame\n",
        "sales_pd_10k = pd.DataFrame()\n",
        "for i in range(0,10):\n",
        "    temp_pd = sales_pd.copy()\n",
        "    ip1 = i + 1\n",
        "    temp_pd['store'] = temp_pd['store'] + (10 * i)\n",
        "    sales_pd_10k = pd.concat([sales_pd_10k, temp_pd])\n",
        "    print('added data frame', ip1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simftWgVpaGo",
        "outputId": "c202a777-0664-4b37-99b2-61f4d8ac7ae6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added data frame 1\n",
            "added data frame 2\n",
            "added data frame 3\n",
            "added data frame 4\n",
            "added data frame 5\n",
            "added data frame 6\n",
            "added data frame 7\n",
            "added data frame 8\n",
            "added data frame 9\n",
            "added data frame 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of unique groups\n",
        "sales_pd_10k[['item', 'store']].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHLMylSg5aKM",
        "outputId": "b915d31c-2017-41f3-fc53-b2e8bd40c158"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "item      50\n",
              "store    100\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have 100 stores x 50 items = 5,000 store-item groups.\n",
        "\n",
        "### 5.2. Prophet x PySpark\n",
        "#### 5.2.1. Create a Spark session\n",
        "Spark Sessions utilize Spark's functions. They are created in the Driver program, which is inside the Master node. \n",
        "\n",
        "Spark uses Master-Slave architecture. Salve nodes execute the tasks assigned by the Master node."
      ],
      "metadata": {
        "id": "L23Aa-B7XTri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark Session - Run it on a standalone mode since it is just a practice\n",
        "# master(): Either yarn or mesos; local[X] when running in standalone\n",
        "# appName(): Name of the application\n",
        "# getOrCreate: returns existing SparkSession; otherwise, create a new one\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4051')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "Ni1i9p58aMz1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.2. Parallelization and structure schema\n",
        "Reading the CSV file with PySpark."
      ],
      "metadata": {
        "id": "Onjwcxr9bc8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv file\n",
        "sales_df = spark.createDataFrame(sales_pd_10k)\n",
        "\n",
        "# Display the schema\n",
        "sales_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF4EHn9TaPW9",
        "outputId": "3a17d7d0-403b-4e53-a044-ab07554a5c96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ds: timestamp (nullable = true)\n",
            " |-- store: long (nullable = true)\n",
            " |-- item: long (nullable = true)\n",
            " |-- y: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to partition all the data based on store and item for parallel processing."
      ],
      "metadata": {
        "id": "pC95jETImFdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition the data\n",
        "sales_df.createOrReplaceTempView(\"item_sales\")\n",
        "sql = \"select * from item_sales\"\n",
        "sales_part = (spark.sql(sql)\\\n",
        "   .repartition(spark.sparkContext.defaultParallelism, \n",
        "   ['store', 'item'])).cache()\n",
        "sales_part.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcO6ICwWW5O9",
        "outputId": "3c7be574-c991-480f-8018-b60488a443af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "InMemoryTableScan [ds#0, store#1L, item#2L, y#3]\n",
            "   +- InMemoryRelation [ds#0, store#1L, item#2L, y#3], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "         +- Exchange hashpartitioning(store#1L, item#2L, 1), false, [id=#11]\n",
            "            +- *(1) Scan ExistingRDD[ds#0,store#1L,item#2L,y#3]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will structure the output of the data. See <a href = \"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html\">here</a> for different types of struct fields.\n"
      ],
      "metadata": {
        "id": "aGm9YTLcmHCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a schema\n",
        "schema = StructType([\n",
        "                     StructField('store', IntegerType()),\n",
        "                     StructField('item', IntegerType()),\n",
        "                     StructField('ds', TimestampType()),\n",
        "                     StructField('y', FloatType()),\n",
        "                     StructField('yhat', DoubleType()),\n",
        "                     StructField('yhat_upper', DoubleType()),\n",
        "                     StructField('yhat_lower', DoubleType()),\n",
        "                     ])  "
      ],
      "metadata": {
        "id": "OMmUC-UYhhBr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.3. Utilize Pandas UDF and PySpark to train multiple models in parallel\n",
        "The next step is to set parameters, fit the model, and predict sales just as we did for 1 forecast model. We are going to build a function and apply that function to all store-item groups. The only difference between this and our previous 1-model forecast is that we are going to utilize Pandas UDF and PySpark to parallelize the process."
      ],
      "metadata": {
        "id": "vZYqQhhfh750"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the Pandas UDF \n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def apply_model(store_pd):\n",
        "  \n",
        "  # instantiate the model and set parameters\n",
        "  model = Prophet(\n",
        "      interval_width=0.95,\n",
        "      growth='linear',\n",
        "      daily_seasonality=False,\n",
        "      weekly_seasonality=True,\n",
        "      yearly_seasonality=True,\n",
        "      seasonality_mode='multiplicative'\n",
        "  )\n",
        "  \n",
        "  # fit the model to historical data\n",
        "  model.fit(store_pd)\n",
        "  \n",
        "  # Create a data frame that lists 90 dates starting from Jan 1 2018\n",
        "  future = model.make_future_dataframe(\n",
        "      periods=90,\n",
        "      freq='d',\n",
        "      include_history=True)\n",
        "  \n",
        "  # Out of sample prediction\n",
        "  future = model.predict(future)\n",
        "\n",
        "  # Create a data frame that contains store, item, y, and yhat\n",
        "  f_pd = future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]\n",
        "  st_pd = store_pd[['ds', 'store', 'item', 'y']]\n",
        "  result_pd = f_pd.join(st_pd.set_index('ds'), on='ds', how='left')\n",
        "  \n",
        "  # fill store and item\n",
        "  result_pd['store'] = store_pd['store'].iloc[0]\n",
        "  result_pd['item'] = store_pd['item'].iloc[0]\n",
        "  #result_pd['store'] = store_pd['store'].fillna(method='ffill')\n",
        "  #result_pd['item'] = store_pd['item'].fillna(method='ffill')\n",
        "  return result_pd[['store', 'item', 'ds', 'y', 'yhat',\n",
        "                    'yhat_upper', 'yhat_lower']]"
      ],
      "metadata": {
        "id": "a0xZR1Hbh5Pr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to all store-items\n",
        "results = sales_part.groupby(['store', 'item']).apply(apply_model)\n",
        "\n",
        "# Print the results - calculate the time to run\n",
        "import timeit\n",
        "start = timeit.default_timer()\n",
        "results.show()\n",
        "stop = timeit.default_timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAtY7_yrhBr",
        "outputId": "e41f3c68-bd1c-4caa-a350-eabd9dcfa577"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  \"more details.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "|store|item|                 ds|   y|              yhat|        yhat_upper|          yhat_lower|\n",
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "|    1|   1|2013-01-01 00:00:00|13.0|10.051272869689301|18.517132295245975|   1.647939275969288|\n",
            "|    1|   1|2013-01-02 00:00:00|11.0|10.528625323821489|18.945047285164023|  2.0535882242744057|\n",
            "|    1|   1|2013-01-03 00:00:00|14.0|11.053264561305632|19.166267433371335|  1.9700010043874727|\n",
            "|    1|   1|2013-01-04 00:00:00|13.0|12.244392640789227|20.201176449280286|   4.116689111099245|\n",
            "|    1|   1|2013-01-05 00:00:00|10.0| 13.78033453999933| 22.59021233927294|    5.38647052961445|\n",
            "|    1|   1|2013-01-06 00:00:00|12.0|14.378950515739104|22.840947052264926|       5.45097483763|\n",
            "|    1|   1|2013-01-07 00:00:00|10.0| 7.872892467507367| 16.46605719095353|-0.18726325030012617|\n",
            "|    1|   1|2013-01-08 00:00:00| 9.0|  9.90131654595669|18.966336803972837|   2.109303690226387|\n",
            "|    1|   1|2013-01-09 00:00:00|12.0|10.315249013275794|18.775958126995757|    2.10490063423655|\n",
            "|    1|   1|2013-01-10 00:00:00| 9.0|10.787017356538591|19.086193156572296|  3.0666868508288707|\n",
            "|    1|   1|2013-01-11 00:00:00| 9.0|11.938934222254371|20.696873134198235|   3.357175944044647|\n",
            "|    1|   1|2013-01-12 00:00:00| 7.0|13.449630508550548| 21.79640051103188|   5.035636631157828|\n",
            "|    1|   1|2013-01-13 00:00:00|10.0| 14.03440099518179|22.682495232273546|   5.439976247054692|\n",
            "|    1|   1|2013-01-14 00:00:00|12.0| 7.510442511052676| 16.18047135066266| -1.0859982006199087|\n",
            "|    1|   1|2013-01-15 00:00:00| 5.0| 9.556668240912103|17.916940418230578|  0.8558159465961486|\n",
            "|    1|   1|2013-01-16 00:00:00| 7.0| 9.997643135111675| 18.27389552583732|   1.824927419618936|\n",
            "|    1|   1|2013-01-17 00:00:00|16.0|10.509012136011599| 19.22242569738084|  1.5899201549511028|\n",
            "|    1|   1|2013-01-18 00:00:00| 7.0|11.713373568083002|19.581336448386864|   3.110647870496671|\n",
            "|    1|   1|2013-01-19 00:00:00|18.0|13.286933306466477|21.272261014882243|   4.524112105165262|\n",
            "|    1|   1|2013-01-20 00:00:00|15.0|13.939857891685119|22.246707519133967|   5.099642350973402|\n",
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the time it took to forecast 500 models\n",
        "print('Time: ', stop - start)   "
      ],
      "metadata": {
        "id": "7HYMOUeKuKui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048f94d3-3e07-464a-b939-42a130cc558e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:  69.43607786799998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It only took 29 seconds to train 500 models and forecast 3 months out!"
      ],
      "metadata": {
        "id": "iQ3K4bl3-geO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.coalesce(1)\n",
        "results.createOrReplaceTempView('forecasted')\n",
        "spark.sql(\"SELECT * FROM forecasted WHERE ITEM==1 AND STORE==1\").show()"
      ],
      "metadata": {
        "id": "ry4L4k1143CE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef3bec5-7bf0-46d2-ca4a-902d45ad68e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "|store|item|                 ds|   y|              yhat|        yhat_upper|          yhat_lower|\n",
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "|    1|   1|2013-01-01 00:00:00|13.0|10.051272869689301|18.517132295245975|   1.647939275969288|\n",
            "|    1|   1|2013-01-02 00:00:00|11.0|10.528625323821489|18.945047285164023|  2.0535882242744057|\n",
            "|    1|   1|2013-01-03 00:00:00|14.0|11.053264561305632|19.166267433371335|  1.9700010043874727|\n",
            "|    1|   1|2013-01-04 00:00:00|13.0|12.244392640789227|20.201176449280286|   4.116689111099245|\n",
            "|    1|   1|2013-01-05 00:00:00|10.0| 13.78033453999933| 22.59021233927294|    5.38647052961445|\n",
            "|    1|   1|2013-01-06 00:00:00|12.0|14.378950515739104|22.840947052264926|       5.45097483763|\n",
            "|    1|   1|2013-01-07 00:00:00|10.0| 7.872892467507367| 16.46605719095353|-0.18726325030012617|\n",
            "|    1|   1|2013-01-08 00:00:00| 9.0|  9.90131654595669|18.966336803972837|   2.109303690226387|\n",
            "|    1|   1|2013-01-09 00:00:00|12.0|10.315249013275794|18.775958126995757|    2.10490063423655|\n",
            "|    1|   1|2013-01-10 00:00:00| 9.0|10.787017356538591|19.086193156572296|  3.0666868508288707|\n",
            "|    1|   1|2013-01-11 00:00:00| 9.0|11.938934222254371|20.696873134198235|   3.357175944044647|\n",
            "|    1|   1|2013-01-12 00:00:00| 7.0|13.449630508550548| 21.79640051103188|   5.035636631157828|\n",
            "|    1|   1|2013-01-13 00:00:00|10.0| 14.03440099518179|22.682495232273546|   5.439976247054692|\n",
            "|    1|   1|2013-01-14 00:00:00|12.0| 7.510442511052676| 16.18047135066266| -1.0859982006199087|\n",
            "|    1|   1|2013-01-15 00:00:00| 5.0| 9.556668240912103|17.916940418230578|  0.8558159465961486|\n",
            "|    1|   1|2013-01-16 00:00:00| 7.0| 9.997643135111675| 18.27389552583732|   1.824927419618936|\n",
            "|    1|   1|2013-01-17 00:00:00|16.0|10.509012136011599| 19.22242569738084|  1.5899201549511028|\n",
            "|    1|   1|2013-01-18 00:00:00| 7.0|11.713373568083002|19.581336448386864|   3.110647870496671|\n",
            "|    1|   1|2013-01-19 00:00:00|18.0|13.286933306466477|21.272261014882243|   4.524112105165262|\n",
            "|    1|   1|2013-01-20 00:00:00|15.0|13.939857891685119|22.246707519133967|   5.099642350973402|\n",
            "+-----+----+-------------------+----+------------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusion\n",
        "In this long post, we went through several topics. We started with identifying trends and seasonality, moved on to building a Prophet model, and scaled the process to model 500 distinct models with PySpark. We didn't get to cover CNN, LSTM, and Seasonal ARIMA but I am planning on adding them in a few days."
      ],
      "metadata": {
        "id": "GFWiCFe7-xEc"
      }
    }
  ]
}