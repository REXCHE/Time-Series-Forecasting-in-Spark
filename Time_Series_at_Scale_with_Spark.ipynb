{"cells":[{"cell_type":"markdown","metadata":{"id":"XDqaPV1K0syV"},"source":["# 5. Implementation of Scalable Demand Forecasting with PySpark in Google Colab\n","Similar to setting up Prophet, PySpark installation can be very difficult at times. However, those tasks are extremely easy Google Colaboratory. \n","\n","First, go to \u003ca href = \"https://research.google.com/colaboratory\"\u003eGoogle Colab\u003c/a\u003e and click \"File\" -\u003e \"New notebook\" to create a new notebook.\n","\n","### 5.1. Preparation\n","#### 5.1.1. Mount to Google Drive\n","For easy access to files, connect the notebook to your Google Drive."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66342,"status":"ok","timestamp":1651333845013,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"scRRK2ujaFyM","outputId":"25f8b7ed-4ba6-49c2-a2c9-d956e3d34ccc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Import library\n","from google.colab import drive\n","\n","# Connect to your google drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"e_UTJL8k1uZP"},"source":["#### 5.1.2. Install PySpark and Prophet\n","Installing PySpark and Prophet only require one line of code for each."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":37731,"status":"ok","timestamp":1651378972504,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"UjTNhmwYaLZL"},"outputs":[],"source":["# Install Java\n","!apt-get install openjdk-8-jdk-headless -qq \u003e /dev/null\n","\n","# Download Spark\n","!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n","\n","# Unzip\n","!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n","\n","# Install spark\n","!pip install -q findspark\n","\n","# Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n","\n","#\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dhduj8z92YIC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Prophet\n","  Downloading prophet-1.0.1.tar.gz (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 2.4 MB/s \n","\u001b[?25hRequirement already satisfied: Cython\u003e=0.22 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.29.28)\n","Collecting cmdstanpy==0.9.68\n","  Downloading cmdstanpy-0.9.68-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.6 MB/s \n","\u001b[?25hRequirement already satisfied: pystan~=2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.19.1.1)\n","Requirement already satisfied: numpy\u003e=1.15.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.21.6)\n","Requirement already satisfied: pandas\u003e=1.0.4 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.3.5)\n","Requirement already satisfied: matplotlib\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (3.2.2)\n","Requirement already satisfied: LunarCalendar\u003e=0.0.9 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.0.9)\n","Requirement already satisfied: convertdate\u003e=2.1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.4.0)\n","Requirement already satisfied: holidays\u003e=0.10.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (0.10.5.2)\n","Requirement already satisfied: setuptools-git\u003e=1.2 in /usr/local/lib/python3.7/dist-packages (from Prophet) (1.2)\n","Requirement already satisfied: python-dateutil\u003e=2.8.0 in /usr/local/lib/python3.7/dist-packages (from Prophet) (2.8.2)\n","Requirement already satisfied: tqdm\u003e=4.36.1 in /usr/local/lib/python3.7/dist-packages (from Prophet) (4.64.0)\n","Collecting ujson\n","  Downloading ujson-5.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: pymeeus\u003c=1,\u003e=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate\u003e=2.1.2-\u003eProphet) (0.5.11)\n","Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays\u003e=0.10.2-\u003eProphet) (2.2.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from holidays\u003e=0.10.2-\u003eProphet) (1.15.0)\n","Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays\u003e=0.10.2-\u003eProphet) (0.2.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from LunarCalendar\u003e=0.0.9-\u003eProphet) (2022.1)\n","Requirement already satisfied: ephem\u003e=3.7.5.3 in /usr/local/lib/python3.7/dist-packages (from LunarCalendar\u003e=0.0.9-\u003eProphet) (4.1.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=2.0.0-\u003eProphet) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=2.0.0-\u003eProphet) (3.0.8)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=2.0.0-\u003eProphet) (1.4.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver\u003e=1.0.1-\u003ematplotlib\u003e=2.0.0-\u003eProphet) (4.2.0)\n","Building wheels for collected packages: Prophet\n","  Building wheel for Prophet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Prophet: filename=prophet-1.0.1-py3-none-any.whl size=6639062 sha256=76e58c3e0efc63042a223f43034db581b9e0bb262ed89520b73c891df3315c7f\n","  Stored in directory: /root/.cache/pip/wheels/4e/a0/1a/02c9ec9e3e9de6bdbb3d769d11992a6926889d71567d6b9b67\n","Successfully built Prophet\n","Installing collected packages: ujson, cmdstanpy, Prophet\n","  Attempting uninstall: cmdstanpy\n","    Found existing installation: cmdstanpy 0.9.5\n","    Uninstalling cmdstanpy-0.9.5:\n","      Successfully uninstalled cmdstanpy-0.9.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fbprophet 0.7.1 requires cmdstanpy==0.9.5, but you have cmdstanpy 0.9.68 which is incompatible.\u001b[0m\n","Successfully installed Prophet-1.0.1 cmdstanpy-0.9.68 ujson-5.2.0\n"]}],"source":["# Install Prophet                                                                                                                                                                                                  \n","!pip install Prophet"]},{"cell_type":"markdown","metadata":{"id":"u4KOO_ZeUp-C"},"source":["#### 5.1.3. Load necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JMaP3Da-UzyI"},"outputs":[],"source":["# Import library\n","import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import pandas_udf, PandasUDFType\n","from pyspark.sql.types import *\n","from prophet import Prophet\n"]},{"cell_type":"markdown","metadata":{"id":"T_7NDC3_PPhZ"},"source":["#### 5.1.4. Upload the CSV file to Google Drive\n","- Click the folder icon in the left menu as shown in the image below.\n","- Although you can save anywhere you wish, I like to save it in the Google Drive Colab Notebook folder. To do so, go to \"content\" -\u003e \"dive\" -\u003e \"MyDrive\" -\u003e Colab Notebooks -\u003e create \"data\" folder\n","- Click the three dots next to \"data\". You can upload the CSV file we saved by clicking \"Upload\"\n","\n","\u003cimg src =\"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab1.JPG?raw=true\"\u003e\n","\u003cimg src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab2.JPG?raw=true\"\u003e\n","\u003cimg src = \"https://github.com/youngdataspace/Time-Series-Forecasting-in-Spark/blob/main/Google%20Colab3.JPG?raw=true\"\u003e\n","\n","#### 5.1.5. Import the CSV file and explore it\n","Import the CSV file we just uploaded to Google Drive.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"64_fumPM2-lA"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 913000 entries, 0 to 912999\n","Data columns (total 4 columns):\n"," #   Column  Non-Null Count   Dtype         \n","---  ------  --------------   -----         \n"," 0   ds      913000 non-null  datetime64[ns]\n"," 1   store   913000 non-null  int64         \n"," 2   item    913000 non-null  int64         \n"," 3   y       913000 non-null  float64       \n","dtypes: datetime64[ns](1), float64(1), int64(2)\n","memory usage: 27.9 MB\n"]}],"source":["# Import the csv file and explore it\n","sales_pd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/store_sales.csv')\n","\n","# Convert ds to datetime\n","sales_pd['ds'] = pd.to_datetime(sales_pd['ds'])\n","\n","# Display info\n","sales_pd.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aq2XcbmLWhh7"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-d88afa49-51f8-4fa1-b7d4-3c3368edeafc\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003estore\u003c/th\u003e\n","      \u003cth\u003eitem\u003c/th\u003e\n","      \u003cth\u003ey\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003ecount\u003c/th\u003e\n","      \u003ctd\u003e913000.000000\u003c/td\u003e\n","      \u003ctd\u003e913000.000000\u003c/td\u003e\n","      \u003ctd\u003e913000.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003emean\u003c/th\u003e\n","      \u003ctd\u003e5.500000\u003c/td\u003e\n","      \u003ctd\u003e25.500000\u003c/td\u003e\n","      \u003ctd\u003e52.250287\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003estd\u003c/th\u003e\n","      \u003ctd\u003e2.872283\u003c/td\u003e\n","      \u003ctd\u003e14.430878\u003c/td\u003e\n","      \u003ctd\u003e28.801144\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003emin\u003c/th\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e25%\u003c/th\u003e\n","      \u003ctd\u003e3.000000\u003c/td\u003e\n","      \u003ctd\u003e13.000000\u003c/td\u003e\n","      \u003ctd\u003e30.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e50%\u003c/th\u003e\n","      \u003ctd\u003e5.500000\u003c/td\u003e\n","      \u003ctd\u003e25.500000\u003c/td\u003e\n","      \u003ctd\u003e47.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e75%\u003c/th\u003e\n","      \u003ctd\u003e8.000000\u003c/td\u003e\n","      \u003ctd\u003e38.000000\u003c/td\u003e\n","      \u003ctd\u003e70.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003emax\u003c/th\u003e\n","      \u003ctd\u003e10.000000\u003c/td\u003e\n","      \u003ctd\u003e50.000000\u003c/td\u003e\n","      \u003ctd\u003e231.000000\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d88afa49-51f8-4fa1-b7d4-3c3368edeafc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d88afa49-51f8-4fa1-b7d4-3c3368edeafc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d88afa49-51f8-4fa1-b7d4-3c3368edeafc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["               store           item              y\n","count  913000.000000  913000.000000  913000.000000\n","mean        5.500000      25.500000      52.250287\n","std         2.872283      14.430878      28.801144\n","min         1.000000       1.000000       0.000000\n","25%         3.000000      13.000000      30.000000\n","50%         5.500000      25.500000      47.000000\n","75%         8.000000      38.000000      70.000000\n","max        10.000000      50.000000     231.000000"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Descriptive statistics\n","sales_pd.describe()"]},{"cell_type":"markdown","metadata":{"id":"L23Aa-B7XTri"},"source":["Looks like we correctly have 1-10 stores and 1-50 items.\n","\n","### 5.2. Prophet x PySpark\n","#### 5.2.1. Create a Spark session\n","Spark Sessions utilize Spark's functions. They are created in the Driver program, which is inside the Master node. \n","\n","Spark uses Master-Slave architecture. Salve nodes execute the tasks assigned by the Master node."]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651348886581,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"Ni1i9p58aMz1"},"outputs":[],"source":["# Create a Spark Session - Run it on a standalone mode since it is just a practice\n","# master(): Either yarn or mesos; local[X] when running in standalone\n","# appName(): Name of the application\n","# getOrCreate: returns existing SparkSession; otherwise, create a new one\n","spark = SparkSession.builder\\\n","        .master(\"local\")\\\n","        .appName(\"Colab\")\\\n","        .getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"Onjwcxr9bc8V"},"source":["#### 5.2.2. Structure schema\n","After reading the CSV file with PySpark we will structure the output of the data. See \u003ca href = \"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html\"\u003ehere\u003c/a\u003e for different types of struct fields.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hF4EHn9TaPW9"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-11-07e9f5d05f8a\u003e\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    +# Read the csv file\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# Read the csv file\n","sales_df = spark.createDataFrame(sales_pd[sales_pd.item==1][sales_pd.store==1])\n","\n","# Display the schema\n","sales_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"McPRReDt5b34"},"outputs":[],"source":["\n","sales_df.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rNDl0XH25StO"},"outputs":[],"source":["results.createOrReplaceTempView(\"item_sales\")\n","queried = spark.sql(\"select * from item_sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BcO6ICwWW5O9"},"outputs":[],"source":["# We need to partition all the data based on store for parallel processing\n","sales_part = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['store'])).cache()\n","sales_part.explain()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1651334069408,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"OMmUC-UYhhBr"},"outputs":[],"source":["# Define a schema\n","schema = StructType([\n","                     StructField('store', IntegerType()),\n","                     StructField('item', IntegerType()),\n","                     StructField('ds', TimestampType()),\n","                     StructField('y', FloatType()),\n","                     StructField('yhat', DoubleType()),\n","                     StructField('yhat_upper', DoubleType()),\n","                     StructField('yhat_lower', DoubleType()),\n","                     ])  "]},{"cell_type":"markdown","metadata":{"id":"vZYqQhhfh750"},"source":["#### 5.2.3. Utilize Pandas UDF and PySpark to train multiple models in parallel\n","The next step is to set parameters, fit the model, and predict sales just as we did for 1 forecast model. We are going to build a function and apply that function to all store-item groups. The only difference between this and our previous 1-model forecast is that we are going to utilize Pandas UDF and PySpark to parallelize the process."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1651334069408,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"a0xZR1Hbh5Pr"},"outputs":[],"source":["# define the Pandas UDF \n","@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n","def apply_model(store_pd):\n","  \n","  # instantiate the model and set parameters\n","  model = Prophet(\n","      interval_width=0.95,\n","      growth='linear',\n","      daily_seasonality=False,\n","      weekly_seasonality=True,\n","      yearly_seasonality=True,\n","      seasonality_mode='multiplicative'\n","  )\n","  \n","  # fit the model to historical data\n","  model.fit(store_pd)\n","  \n","  # Create a data frame that lists 90 dates starting from Jan 1 2018\n","  future = model.make_future_dataframe(\n","      periods=90,\n","      freq='d',\n","      include_history=True)\n","  \n","  # Out of sample prediction\n","  future = model.predict(future)\n","\n","  # Create a data frame that contains store, item, y, and yhat\n","  f_pd = future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']]\n","  st_pd = store_pd[['ds', 'store', 'item', 'y']]\n","  result_pd = f_pd.join(st_pd.set_index('ds'), on='ds', how='left')\n","  \n","  # fill store and item\n","  result_pd['store'] = store_pd['store'].iloc[0]\n","  result_pd['item'] = store_pd['item'].iloc[0]\n","  #result_pd['store'] = store_pd['store'].fillna(method='ffill')\n","  #result_pd['item'] = store_pd['item'].fillna(method='ffill')\n","  return result_pd[['store', 'item', 'ds', 'y', 'yhat',\n","                    'yhat_upper', 'yhat_lower']]"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32410,"status":"ok","timestamp":1651334101809,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"jkAtY7_yrhBr"},"outputs":[],"source":["# Apply the function to all store-items\n","results = sales_df.groupby(['store', 'item']).apply(apply_model)\n","\n","# Print the results - calculate the time to run\n","import timeit\n","start = timeit.default_timer()\n","results.show()\n","stop = timeit.default_timer()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651334101810,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"7HYMOUeKuKui"},"outputs":[],"source":["# Print the time it took to forecast 500 models\n","print('Time: ', stop - start)   "]},{"cell_type":"markdown","metadata":{"id":"iQ3K4bl3-geO"},"source":["It only took 29 seconds to train 500 models and forecast 3 months out!"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1651334101810,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"z_mm2XySFmSX","outputId":"8e3e81ec-1397-4e35-fb92-dd4d7f06c5e9"},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["type(results) "]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":426,"status":"ok","timestamp":1651335435670,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"xHJIawuTPqND"},"outputs":[],"source":["results.createOrReplaceTempView(\"item_sales\")\n","queried = spark.sql(\"select * from item_sales where store==1 AND item==1\")"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":276,"status":"ok","timestamp":1651335969208,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"4aO-6oj4SwvN"},"outputs":[],"source":["# Enable Arrow-based columnar data transfers\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":41911,"status":"error","timestamp":1651335953292,"user":{"displayName":"Young S. Yoon","userId":"12960390886179462185"},"user_tz":420},"id":"6s6nPkOsQxzB","outputId":"945b5a3f-03e9-41a1-d27d-1ebd3d8dfb42"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR:root:KeyboardInterrupt while sending command.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n","    return self._sock.recv_into(b)\n","KeyboardInterrupt\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-25-9acf2f71390e\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mresult_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 157\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \"\"\"\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 693\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["result_pdf = results.select(\"*\").toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KASMtDyMPtFz"},"outputs":[],"source":["sqlContext.sql(\"SELECT domain_userid, COUNT(*) AS count FROM events GROUP BY domain_userid\").show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLfj451IYNWz"},"outputs":[],"source":["final_df = final_df.set_index('ds')\n","final_df.query('store_id == 25')[['y', 'yhat']].plot()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GFWiCFe7-xEc"},"source":["# 6. Conclusion\n","In this long post, we went through several topics. We started with identifying trends and seasonality, moved on to building a Prophet model, and scaled the process to model 500 distinct models with PySpark. We didn't get to cover CNN, LSTM, and Seasonal ARIMA but I am planning on adding them in a few days."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPqIAO9vH4TqKl+4aWfQih4","mount_file_id":"17WB4URrt6ypAF3YwuG1lRhE7_XvcXlQ_","name":"Time_Series_at_Scale_with_Spark.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}